"""
è‚¡ç¥¨åˆ†ææ‰§è¡Œå·¥å…·
"""

import sys
import os
import uuid
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
import streamlit as st

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger, get_logger_manager
logger = get_logger('web')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# ç¡®ä¿ç¯å¢ƒå˜é‡æ­£ç¡®åŠ è½½
load_dotenv(project_root / ".env", override=True)

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import setup_web_logging
logger = setup_web_logging()

# æ·»åŠ é…ç½®ç®¡ç†å™¨
try:
    from tradingagents.config.config_manager import token_tracker
    TOKEN_TRACKING_ENABLED = True
    logger.info("âœ… Tokenè·Ÿè¸ªåŠŸèƒ½å·²å¯ç”¨")
except ImportError:
    TOKEN_TRACKING_ENABLED = False
    logger.warning("âš ï¸ Tokenè·Ÿè¸ªåŠŸèƒ½æœªå¯ç”¨")

def translate_analyst_labels(text):
    """å°†åˆ†æå¸ˆçš„è‹±æ–‡æ ‡ç­¾è½¬æ¢ä¸ºä¸­æ–‡"""
    if not text:
        return text

    # åˆ†æå¸ˆæ ‡ç­¾ç¿»è¯‘æ˜ å°„
    translations = {
        'Bull Analyst:': 'çœ‹æ¶¨åˆ†æå¸ˆ:',
        'Bear Analyst:': 'çœ‹è·Œåˆ†æå¸ˆ:',
        'Risky Analyst:': 'æ¿€è¿›é£é™©åˆ†æå¸ˆ:',
        'Safe Analyst:': 'ä¿å®ˆé£é™©åˆ†æå¸ˆ:',
        'Neutral Analyst:': 'ä¸­æ€§é£é™©åˆ†æå¸ˆ:',
        'Research Manager:': 'ç ”ç©¶ç»ç†:',
        'Portfolio Manager:': 'æŠ•èµ„ç»„åˆç»ç†:',
        'Risk Judge:': 'é£é™©ç®¡ç†å§”å‘˜ä¼š:',
        'Trader:': 'äº¤æ˜“å‘˜:'
    }

    # æ›¿æ¢æ‰€æœ‰è‹±æ–‡æ ‡ç­¾
    for english, chinese in translations.items():
        text = text.replace(english, chinese)

    return text

def extract_risk_assessment(state):
    """ä»åˆ†æçŠ¶æ€ä¸­æå–é£é™©è¯„ä¼°æ•°æ®"""
    try:
        risk_debate_state = state.get('risk_debate_state', {})

        if not risk_debate_state:
            return None

        # æå–å„ä¸ªé£é™©åˆ†æå¸ˆçš„è§‚ç‚¹å¹¶è¿›è¡Œä¸­æ–‡åŒ–
        risky_analysis = translate_analyst_labels(risk_debate_state.get('risky_history', ''))
        safe_analysis = translate_analyst_labels(risk_debate_state.get('safe_history', ''))
        neutral_analysis = translate_analyst_labels(risk_debate_state.get('neutral_history', ''))
        judge_decision = translate_analyst_labels(risk_debate_state.get('judge_decision', ''))

        # æ ¼å¼åŒ–é£é™©è¯„ä¼°æŠ¥å‘Š
        risk_assessment = f"""
## âš ï¸ é£é™©è¯„ä¼°æŠ¥å‘Š

### ğŸ”´ æ¿€è¿›é£é™©åˆ†æå¸ˆè§‚ç‚¹
{risky_analysis if risky_analysis else 'æš‚æ— æ¿€è¿›é£é™©åˆ†æ'}

### ğŸŸ¡ ä¸­æ€§é£é™©åˆ†æå¸ˆè§‚ç‚¹
{neutral_analysis if neutral_analysis else 'æš‚æ— ä¸­æ€§é£é™©åˆ†æ'}

### ğŸŸ¢ ä¿å®ˆé£é™©åˆ†æå¸ˆè§‚ç‚¹
{safe_analysis if safe_analysis else 'æš‚æ— ä¿å®ˆé£é™©åˆ†æ'}

### ğŸ›ï¸ é£é™©ç®¡ç†å§”å‘˜ä¼šæœ€ç»ˆå†³è®®
{judge_decision if judge_decision else 'æš‚æ— é£é™©ç®¡ç†å†³è®®'}

---
*é£é™©è¯„ä¼°åŸºäºå¤šè§’åº¦åˆ†æï¼Œè¯·ç»“åˆä¸ªäººé£é™©æ‰¿å—èƒ½åŠ›åšå‡ºæŠ•èµ„å†³ç­–*
        """.strip()

        return risk_assessment

    except (KeyError, AttributeError, TypeError) as e:
        logger.info(f"æå–é£é™©è¯„ä¼°æ•°æ®æ—¶å‡ºé”™ï¼Œæ•°æ®æ ¼å¼å¼‚å¸¸: {e}")
        return None
    except ValueError as e:
        logger.info(f"æå–é£é™©è¯„ä¼°æ•°æ®æ—¶å‡ºé”™ï¼Œæ•°å€¼è§£æå¤±è´¥: {e}")
        return None

def run_stock_analysis(stock_symbol, analysis_date, analysts, research_depth, llm_provider, llm_model, market_type="Aè‚¡", progress_callback=None):
    """æ‰§è¡Œè‚¡ç¥¨åˆ†æ

    Args:
        stock_symbol: è‚¡ç¥¨ä»£ç 
        analysis_date: åˆ†ææ—¥æœŸ
        analysts: åˆ†æå¸ˆåˆ—è¡¨
        research_depth: ç ”ç©¶æ·±åº¦
        llm_provider: LLMæä¾›å•† (dashscope/deepseek/google)
        llm_model: å¤§æ¨¡å‹åç§°
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œç”¨äºæ›´æ–°UIçŠ¶æ€
    """

    def update_progress(message, step=None, total_steps=None):
        """æ›´æ–°è¿›åº¦"""
        if progress_callback:
            progress_callback(message, step, total_steps)
        logger.info(f"[è¿›åº¦] {message}")

    # ç”Ÿæˆä¼šè¯IDç”¨äºTokenè·Ÿè¸ªå’Œæ—¥å¿—å…³è”
    session_id = f"analysis_{uuid.uuid4().hex[:8]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    # 1. æ•°æ®é¢„è·å–å’ŒéªŒè¯é˜¶æ®µ
    update_progress("ğŸ” éªŒè¯è‚¡ç¥¨ä»£ç å¹¶é¢„è·å–æ•°æ®...", 1, 10)

    try:
        from tradingagents.utils.stock_validator import prepare_stock_data

        # é¢„è·å–è‚¡ç¥¨æ•°æ®ï¼ˆé»˜è®¤30å¤©å†å²æ•°æ®ï¼‰
        preparation_result = prepare_stock_data(
            stock_code=stock_symbol,
            market_type=market_type,
            period_days=30,  # å¯ä»¥æ ¹æ®research_depthè°ƒæ•´
            analysis_date=analysis_date
        )

        if not preparation_result.is_valid:
            error_msg = f"âŒ è‚¡ç¥¨æ•°æ®éªŒè¯å¤±è´¥: {preparation_result.error_message}"
            update_progress(error_msg)
            logger.error(f"[{session_id}] {error_msg}")

            return {
                'success': False,
                'error': preparation_result.error_message,
                'suggestion': preparation_result.suggestion,
                'stock_symbol': stock_symbol,
                'analysis_date': analysis_date,
                'session_id': session_id
            }

        # æ•°æ®é¢„è·å–æˆåŠŸ
        success_msg = f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {preparation_result.stock_name} ({preparation_result.market_type})"
        update_progress(success_msg)  # ä½¿ç”¨æ™ºèƒ½æ£€æµ‹ï¼Œä¸å†ç¡¬ç¼–ç æ­¥éª¤
        logger.info(f"[{session_id}] {success_msg}")
        logger.info(f"[{session_id}] ç¼“å­˜çŠ¶æ€: {preparation_result.cache_status}")

    except ImportError as e:
        error_msg = f"âŒ æ•°æ®é¢„è·å–æ¨¡å—å¯¼å…¥å¤±è´¥: {str(e)}"
        update_progress(error_msg)
    except ConnectionError as e:
        error_msg = f"âŒ æ•°æ®é¢„è·å–ç½‘ç»œè¿æ¥å¤±è´¥: {str(e)}"
        update_progress(error_msg)
    except ValueError as e:
        error_msg = f"âŒ æ•°æ®é¢„è·å–å‚æ•°é”™è¯¯: {str(e)}"
        update_progress(error_msg)
    except RuntimeError as e:
        error_msg = f"âŒ æ•°æ®é¢„è·å–è¿è¡Œæ—¶é”™è¯¯: {str(e)}"
        update_progress(error_msg)
        logger.error(f"[{session_id}] {error_msg}")

        return {
            'success': False,
            'error': error_msg,
            'suggestion': "è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•",
            'stock_symbol': stock_symbol,
            'analysis_date': analysis_date,
            'session_id': session_id
        }

    # è®°å½•åˆ†æå¼€å§‹çš„è¯¦ç»†æ—¥å¿—
    logger_manager = get_logger_manager()
    import time
    analysis_start_time = time.time()

    logger_manager.log_analysis_start(
        logger, stock_symbol, "comprehensive_analysis", session_id
    )

    logger.info(f"ğŸš€ [åˆ†æå¼€å§‹] è‚¡ç¥¨åˆ†æå¯åŠ¨",
               extra={
                   'stock_symbol': stock_symbol,
                   'analysis_date': analysis_date,
                   'analysts': analysts,
                   'research_depth': research_depth,
                   'llm_provider': llm_provider,
                   'llm_model': llm_model,
                   'market_type': market_type,
                   'session_id': session_id,
                   'event_type': 'web_analysis_start'
               })

    update_progress("ğŸš€ å¼€å§‹è‚¡ç¥¨åˆ†æ...")

    # ä¼°ç®—Tokenä½¿ç”¨ï¼ˆç”¨äºæˆæœ¬é¢„ä¼°ï¼‰
    if TOKEN_TRACKING_ENABLED:
        estimated_input = 2000 * len(analysts)  # ä¼°ç®—æ¯ä¸ªåˆ†æå¸ˆ2000ä¸ªè¾“å…¥token
        estimated_output = 1000 * len(analysts)  # ä¼°ç®—æ¯ä¸ªåˆ†æå¸ˆ1000ä¸ªè¾“å‡ºtoken
        estimated_cost = token_tracker.estimate_cost(llm_provider, llm_model, estimated_input, estimated_output)

        update_progress(f"ğŸ’° é¢„ä¼°åˆ†ææˆæœ¬: Â¥{estimated_cost:.4f}")

    # éªŒè¯ç¯å¢ƒå˜é‡
    update_progress("æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®...")
    dashscope_key = os.getenv("DASHSCOPE_API_KEY")
    finnhub_key = os.getenv("FINNHUB_API_KEY")

    logger.info(f"ç¯å¢ƒå˜é‡æ£€æŸ¥:")
    logger.info(f"  DASHSCOPE_API_KEY: {'å·²è®¾ç½®' if dashscope_key else 'æœªè®¾ç½®'}")
    logger.info(f"  FINNHUB_API_KEY: {'å·²è®¾ç½®' if finnhub_key else 'æœªè®¾ç½®'}")

    if not dashscope_key:
        raise ValueError("DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
    if not finnhub_key:
        raise ValueError("FINNHUB_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")

    update_progress("ç¯å¢ƒå˜é‡éªŒè¯é€šè¿‡")

    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from tradingagents.graph.trading_graph import TradingAgentsGraph
        from tradingagents.default_config import DEFAULT_CONFIG

        # åˆ›å»ºé…ç½®
        update_progress("é…ç½®åˆ†æå‚æ•°...")
        config = DEFAULT_CONFIG.copy()
        config["llm_provider"] = llm_provider
        config["deep_think_llm"] = llm_model
        config["quick_think_llm"] = llm_model
        # æ ¹æ®ç ”ç©¶æ·±åº¦è°ƒæ•´é…ç½®
        if research_depth == 1:  # 1çº§ - å¿«é€Ÿåˆ†æ
            config["max_debate_rounds"] = 1
            config["max_risk_discuss_rounds"] = 1
            # ä¿æŒå†…å­˜åŠŸèƒ½å¯ç”¨ï¼Œå› ä¸ºå†…å­˜æ“ä½œå¼€é”€å¾ˆå°ä½†èƒ½æ˜¾è‘—æå‡åˆ†æè´¨é‡
            config["memory_enabled"] = True

            # ç»Ÿä¸€ä½¿ç”¨åœ¨çº¿å·¥å…·ï¼Œé¿å…ç¦»çº¿å·¥å…·çš„å„ç§é—®é¢˜
            config["online_tools"] = True  # æ‰€æœ‰å¸‚åœºéƒ½ä½¿ç”¨ç»Ÿä¸€å·¥å…·
            logger.info(f"ğŸ”§ [å¿«é€Ÿåˆ†æ] {market_type}ä½¿ç”¨ç»Ÿä¸€å·¥å…·ï¼Œç¡®ä¿æ•°æ®æºæ­£ç¡®å’Œç¨³å®šæ€§")
            if llm_provider == "dashscope":
                config["quick_think_llm"] = "qwen-turbo"  # ä½¿ç”¨æœ€å¿«æ¨¡å‹
                config["deep_think_llm"] = "qwen-plus"
            elif llm_provider == "deepseek":
                config["quick_think_llm"] = "deepseek-chat"  # DeepSeekåªæœ‰ä¸€ä¸ªæ¨¡å‹
                config["deep_think_llm"] = "deepseek-chat"
        elif research_depth == 2:  # 2çº§ - åŸºç¡€åˆ†æ
            config["max_debate_rounds"] = 1
            config["max_risk_discuss_rounds"] = 1
            config["memory_enabled"] = True
            config["online_tools"] = True
            if llm_provider == "dashscope":
                config["quick_think_llm"] = "qwen-plus"
                config["deep_think_llm"] = "qwen-plus"
            elif llm_provider == "deepseek":
                config["quick_think_llm"] = "deepseek-chat"
                config["deep_think_llm"] = "deepseek-chat"
            elif llm_provider == "openai":
                config["quick_think_llm"] = llm_model
                config["deep_think_llm"] = llm_model
            elif llm_provider == "openai":
                config["quick_think_llm"] = llm_model
                config["deep_think_llm"] = llm_model
            elif llm_provider == "openai":
                config["quick_think_llm"] = llm_model
                config["deep_think_llm"] = llm_model
            elif llm_provider == "openai":
                config["quick_think_llm"] = llm_model
                config["deep_think_llm"] = llm_model
            elif llm_provider == "openai":
                config["quick_think_llm"] = llm_model
                config["deep_think_llm"] = llm_model
        elif research_depth == 3:  # 3çº§ - æ ‡å‡†åˆ†æ (é»˜è®¤)
            config["max_debate_rounds"] = 1
            config["max_risk_discuss_rounds"] = 2
            config["memory_enabled"] = True
            config["online_tools"] = True
            if llm_provider == "dashscope":
                config["quick_think_llm"] = "qwen-plus"
                config["deep_think_llm"] = "qwen3-max"
            elif llm_provider == "deepseek":
                config["quick_think_llm"] = "deepseek-chat"
                config["deep_think_llm"] = "deepseek-chat"
        elif research_depth == 4:  # 4çº§ - æ·±åº¦åˆ†æ
            config["max_debate_rounds"] = 2
            config["max_risk_discuss_rounds"] = 2
            config["memory_enabled"] = True
            config["online_tools"] = True
            if llm_provider == "dashscope":
                config["quick_think_llm"] = "qwen-plus"
                config["deep_think_llm"] = "qwen3-max"
            elif llm_provider == "deepseek":
                config["quick_think_llm"] = "deepseek-chat"
                config["deep_think_llm"] = "deepseek-chat"
        else:  # 5çº§ - å…¨é¢åˆ†æ
            config["max_debate_rounds"] = 3
            config["max_risk_discuss_rounds"] = 3
            config["memory_enabled"] = True
            config["online_tools"] = True
            if llm_provider == "dashscope":
                config["quick_think_llm"] = "qwen3-max"
                config["deep_think_llm"] = "qwen3-max"
            elif llm_provider == "deepseek":
                config["quick_think_llm"] = "deepseek-chat"
                config["deep_think_llm"] = "deepseek-chat"

        # æ ¹æ®LLMæä¾›å•†è®¾ç½®ä¸åŒçš„é…ç½®
        if llm_provider == "dashscope":
            config["backend_url"] = "https://dashscope.aliyuncs.com/api/v1"
        elif llm_provider == "deepseek":
            config["backend_url"] = "https://api.deepseek.com"
        elif llm_provider == "qianfan":
            # åƒå¸†ï¼ˆæ–‡å¿ƒä¸€è¨€ï¼‰é…ç½®
            config["backend_url"] = "https://aip.baidubce.com"
            # æ ¹æ®ç ”ç©¶æ·±åº¦è®¾ç½®åƒå¸†æ¨¡å‹
            if research_depth <= 2:  # å¿«é€Ÿå’ŒåŸºç¡€åˆ†æ
                config["quick_think_llm"] = "ernie-3.5-8k"
                config["deep_think_llm"] = "ernie-3.5-8k"
            elif research_depth <= 4:  # æ ‡å‡†å’Œæ·±åº¦åˆ†æ
                config["quick_think_llm"] = "ernie-3.5-8k"
                config["deep_think_llm"] = "ernie-4.0-turbo-8k"
            else:  # å…¨é¢åˆ†æ
                config["quick_think_llm"] = "ernie-4.0-turbo-8k"
                config["deep_think_llm"] = "ernie-4.0-turbo-8k"
            
            logger.info(f"ğŸ¤– [åƒå¸†] å¿«é€Ÿæ¨¡å‹: {config['quick_think_llm']}")
            logger.info(f"ğŸ¤– [åƒå¸†] æ·±åº¦æ¨¡å‹: {config['deep_think_llm']}")
        elif llm_provider == "google":
            # Google AIä¸éœ€è¦backend_urlï¼Œä½¿ç”¨é»˜è®¤çš„OpenAIæ ¼å¼
            config["backend_url"] = "https://api.openai.com/v1"
            
            # æ ¹æ®ç ”ç©¶æ·±åº¦ä¼˜åŒ–Googleæ¨¡å‹é€‰æ‹©
            if research_depth == 1:  # å¿«é€Ÿåˆ†æ - ä½¿ç”¨æœ€å¿«æ¨¡å‹
                config["quick_think_llm"] = "gemini-2.5-flash-lite-preview-06-17"  # 1.45s
                config["deep_think_llm"] = "gemini-2.0-flash"  # 1.87s
            elif research_depth == 2:  # åŸºç¡€åˆ†æ - ä½¿ç”¨å¿«é€Ÿæ¨¡å‹
                config["quick_think_llm"] = "gemini-2.0-flash"  # 1.87s
                config["deep_think_llm"] = "gemini-1.5-pro"  # 2.25s
            elif research_depth == 3:  # æ ‡å‡†åˆ†æ - å¹³è¡¡æ€§èƒ½
                config["quick_think_llm"] = "gemini-1.5-pro"  # 2.25s
                config["deep_think_llm"] = "gemini-2.5-flash"  # 2.73s
            elif research_depth == 4:  # æ·±åº¦åˆ†æ - ä½¿ç”¨å¼ºå¤§æ¨¡å‹
                config["quick_think_llm"] = "gemini-2.5-flash"  # 2.73s
                config["deep_think_llm"] = "gemini-2.5-pro"  # 16.68s
            else:  # å…¨é¢åˆ†æ - ä½¿ç”¨æœ€å¼ºæ¨¡å‹
                config["quick_think_llm"] = "gemini-2.5-pro"  # 16.68s
                config["deep_think_llm"] = "gemini-2.5-pro"  # 16.68s
            
            logger.info(f"ğŸ¤– [Google AI] å¿«é€Ÿæ¨¡å‹: {config['quick_think_llm']}")
            logger.info(f"ğŸ¤– [Google AI] æ·±åº¦æ¨¡å‹: {config['deep_think_llm']}")
        elif llm_provider == "openai":
            # OpenAIå®˜æ–¹API
            config["backend_url"] = "https://api.openai.com/v1"
            logger.info(f"ğŸ¤– [OpenAI] ä½¿ç”¨æ¨¡å‹: {llm_model}")
            logger.info(f"ğŸ¤– [OpenAI] APIç«¯ç‚¹: https://api.openai.com/v1")
        elif llm_provider == "openrouter":
            # OpenRouterä½¿ç”¨OpenAIå…¼å®¹API
            config["backend_url"] = "https://openrouter.ai/api/v1"
            logger.info(f"ğŸŒ [OpenRouter] ä½¿ç”¨æ¨¡å‹: {llm_model}")
            logger.info(f"ğŸŒ [OpenRouter] APIç«¯ç‚¹: https://openrouter.ai/api/v1")
        elif llm_provider == "siliconflow":
            config["backend_url"] = "https://api.siliconflow.cn/v1"
            logger.info(f"ğŸŒ [SiliconFlow] ä½¿ç”¨æ¨¡å‹: {llm_model}")
            logger.info(f"ğŸŒ [SiliconFlow] APIç«¯ç‚¹: https://api.siliconflow.cn/v1")
        elif llm_provider == "custom_openai":
            # è‡ªå®šä¹‰OpenAIç«¯ç‚¹
            custom_base_url = st.session_state.get("custom_openai_base_url", "https://api.openai.com/v1")
            config["backend_url"] = custom_base_url
            config["custom_openai_base_url"] = custom_base_url
            logger.info(f"ğŸ”§ [è‡ªå®šä¹‰OpenAI] ä½¿ç”¨æ¨¡å‹: {llm_model}")
            logger.info(f"ğŸ”§ [è‡ªå®šä¹‰OpenAI] APIç«¯ç‚¹: {custom_base_url}")

        # ä¿®å¤è·¯å¾„é—®é¢˜ - ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®
        # æ•°æ®ç›®å½•ï¼šä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        if not config.get("data_dir") or config["data_dir"] == "./data":
            env_data_dir = os.getenv("TRADINGAGENTS_DATA_DIR")
            if env_data_dir:
                # å¦‚æœç¯å¢ƒå˜é‡æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•è§£æ
                if not os.path.isabs(env_data_dir):
                    config["data_dir"] = str(project_root / env_data_dir)
                else:
                    config["data_dir"] = env_data_dir
            else:
                config["data_dir"] = str(project_root / "data")

        # ç»“æœç›®å½•ï¼šä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        if not config.get("results_dir") or config["results_dir"] == "./results":
            env_results_dir = os.getenv("TRADINGAGENTS_RESULTS_DIR")
            if env_results_dir:
                # å¦‚æœç¯å¢ƒå˜é‡æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•è§£æ
                if not os.path.isabs(env_results_dir):
                    config["results_dir"] = str(project_root / env_results_dir)
                else:
                    config["results_dir"] = env_results_dir
            else:
                config["results_dir"] = str(project_root / "results")

        # ç¼“å­˜ç›®å½•ï¼šä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        if not config.get("data_cache_dir"):
            env_cache_dir = os.getenv("TRADINGAGENTS_CACHE_DIR")
            if env_cache_dir:
                # å¦‚æœç¯å¢ƒå˜é‡æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•è§£æ
                if not os.path.isabs(env_cache_dir):
                    config["data_cache_dir"] = str(project_root / env_cache_dir)
                else:
                    config["data_cache_dir"] = env_cache_dir
            else:
                config["data_cache_dir"] = str(project_root / "tradingagents" / "dataflows" / "data_cache")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        update_progress("ğŸ“ åˆ›å»ºå¿…è¦çš„ç›®å½•...")
        os.makedirs(config["data_dir"], exist_ok=True)
        os.makedirs(config["results_dir"], exist_ok=True)
        os.makedirs(config["data_cache_dir"], exist_ok=True)

        logger.info(f"ğŸ“ ç›®å½•é…ç½®:")
        logger.info(f"  - æ•°æ®ç›®å½•: {config['data_dir']}")
        logger.info(f"  - ç»“æœç›®å½•: {config['results_dir']}")
        logger.info(f"  - ç¼“å­˜ç›®å½•: {config['data_cache_dir']}")
        logger.info(f"  - ç¯å¢ƒå˜é‡ TRADINGAGENTS_RESULTS_DIR: {os.getenv('TRADINGAGENTS_RESULTS_DIR', 'æœªè®¾ç½®')}")

        logger.info(f"ä½¿ç”¨é…ç½®: {config}")
        logger.info(f"åˆ†æå¸ˆåˆ—è¡¨: {analysts}")
        logger.info(f"è‚¡ç¥¨ä»£ç : {stock_symbol}")
        logger.info(f"åˆ†ææ—¥æœŸ: {analysis_date}")

        # æ ¹æ®å¸‚åœºç±»å‹è°ƒæ•´è‚¡ç¥¨ä»£ç æ ¼å¼
        logger.debug(f"ğŸ” [RUNNER DEBUG] ===== è‚¡ç¥¨ä»£ç æ ¼å¼åŒ– =====")
        logger.debug(f"ğŸ” [RUNNER DEBUG] åŸå§‹è‚¡ç¥¨ä»£ç : '{stock_symbol}'")
        logger.debug(f"ğŸ” [RUNNER DEBUG] å¸‚åœºç±»å‹: '{market_type}'")

        if market_type == "Aè‚¡":
            # Aè‚¡ä»£ç ä¸éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œä¿æŒåŸæ ·
            formatted_symbol = stock_symbol
            logger.debug(f"ğŸ” [RUNNER DEBUG] Aè‚¡ä»£ç ä¿æŒåŸæ ·: '{formatted_symbol}'")
            update_progress(f"ğŸ‡¨ğŸ‡³ å‡†å¤‡åˆ†æAè‚¡: {formatted_symbol}")
        elif market_type == "æ¸¯è‚¡":
            # æ¸¯è‚¡ä»£ç è½¬ä¸ºå¤§å†™ï¼Œç¡®ä¿.HKåç¼€
            formatted_symbol = stock_symbol.upper()
            if not formatted_symbol.endswith('.HK'):
                # å¦‚æœæ˜¯çº¯æ•°å­—ï¼Œæ·»åŠ .HKåç¼€
                if formatted_symbol.isdigit():
                    formatted_symbol = f"{formatted_symbol.zfill(4)}.HK"
            update_progress(f"ğŸ‡­ğŸ‡° å‡†å¤‡åˆ†ææ¸¯è‚¡: {formatted_symbol}")
        else:
            # ç¾è‚¡ä»£ç è½¬ä¸ºå¤§å†™
            formatted_symbol = stock_symbol.upper()
            logger.debug(f"ğŸ” [RUNNER DEBUG] ç¾è‚¡ä»£ç è½¬å¤§å†™: '{stock_symbol}' -> '{formatted_symbol}'")
            update_progress(f"ğŸ‡ºğŸ‡¸ å‡†å¤‡åˆ†æç¾è‚¡: {formatted_symbol}")

        logger.debug(f"ğŸ” [RUNNER DEBUG] æœ€ç»ˆä¼ é€’ç»™åˆ†æå¼•æ“çš„è‚¡ç¥¨ä»£ç : '{formatted_symbol}'")

        # åˆå§‹åŒ–äº¤æ˜“å›¾
        update_progress("ğŸ”§ åˆå§‹åŒ–åˆ†æå¼•æ“...")
        graph = TradingAgentsGraph(analysts, config=config, debug=False)

        # æ‰§è¡Œåˆ†æ
        update_progress(f"ğŸ“Š å¼€å§‹åˆ†æ {formatted_symbol} è‚¡ç¥¨ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´...")
        logger.debug(f"ğŸ” [RUNNER DEBUG] ===== è°ƒç”¨graph.propagate =====")
        logger.debug(f"ğŸ” [RUNNER DEBUG] ä¼ é€’ç»™graph.propagateçš„å‚æ•°:")
        logger.debug(f"ğŸ” [RUNNER DEBUG]   symbol: '{formatted_symbol}'")
        logger.debug(f"ğŸ” [RUNNER DEBUG]   date: '{analysis_date}'")

        state, decision = graph.propagate(formatted_symbol, analysis_date)

        # è°ƒè¯•ä¿¡æ¯
        logger.debug(f"ğŸ” [DEBUG] åˆ†æå®Œæˆï¼Œdecisionç±»å‹: {type(decision)}")
        logger.debug(f"ğŸ” [DEBUG] decisionå†…å®¹: {decision}")

        # æ ¼å¼åŒ–ç»“æœ
        update_progress("ğŸ“‹ åˆ†æå®Œæˆï¼Œæ­£åœ¨æ•´ç†ç»“æœ...")

        # æå–é£é™©è¯„ä¼°æ•°æ®
        risk_assessment = extract_risk_assessment(state)

        # å°†é£é™©è¯„ä¼°æ·»åŠ åˆ°çŠ¶æ€ä¸­
        if risk_assessment:
            state['risk_assessment'] = risk_assessment

        # è®°å½•Tokenä½¿ç”¨ï¼ˆå®é™…ä½¿ç”¨é‡ï¼Œè¿™é‡Œä½¿ç”¨ä¼°ç®—å€¼ï¼‰
        if TOKEN_TRACKING_ENABLED:
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›å€¼åº”è¯¥ä»LLMå“åº”ä¸­è·å–
            # è¿™é‡Œä½¿ç”¨åŸºäºåˆ†æå¸ˆæ•°é‡å’Œç ”ç©¶æ·±åº¦çš„ä¼°ç®—
            actual_input_tokens = len(analysts) * (1500 if research_depth == "å¿«é€Ÿ" else 2500 if research_depth == "æ ‡å‡†" else 4000)
            actual_output_tokens = len(analysts) * (800 if research_depth == "å¿«é€Ÿ" else 1200 if research_depth == "æ ‡å‡†" else 2000)

            usage_record = token_tracker.track_usage(
                provider=llm_provider,
                model_name=llm_model,
                input_tokens=actual_input_tokens,
                output_tokens=actual_output_tokens,
                session_id=session_id,
                analysis_type=f"{market_type}_analysis"
            )

            if usage_record:
                update_progress(f"ğŸ’° è®°å½•ä½¿ç”¨æˆæœ¬: Â¥{usage_record.cost:.4f}")

        results = {
            'stock_symbol': stock_symbol,
            'analysis_date': analysis_date,
            'analysts': analysts,
            'research_depth': research_depth,
            'llm_provider': llm_provider,
            'llm_model': llm_model,
            'state': state,
            'decision': decision,
            'success': True,
            'error': None,
            'session_id': session_id if TOKEN_TRACKING_ENABLED else None
        }

        # è®°å½•åˆ†æå®Œæˆçš„è¯¦ç»†æ—¥å¿—
        analysis_duration = time.time() - analysis_start_time

        # è®¡ç®—æ€»æˆæœ¬ï¼ˆå¦‚æœæœ‰Tokenè·Ÿè¸ªï¼‰
        total_cost = 0.0
        if TOKEN_TRACKING_ENABLED:
            try:
                total_cost = token_tracker.get_session_cost(session_id)
            except:
                pass

        logger_manager.log_analysis_complete(
            logger, stock_symbol, "comprehensive_analysis", session_id,
            analysis_duration, total_cost
        )

        logger.info(f"âœ… [åˆ†æå®Œæˆ] è‚¡ç¥¨åˆ†ææˆåŠŸå®Œæˆ",
                   extra={
                       'stock_symbol': stock_symbol,
                       'session_id': session_id,
                       'duration': analysis_duration,
                       'total_cost': total_cost,
                       'analysts_used': analysts,
                       'success': True,
                       'event_type': 'web_analysis_complete'
                   })

        # ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æœ¬åœ°å’ŒMongoDB
        try:
            update_progress("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ†ææŠ¥å‘Š...")
            from .report_exporter import save_analysis_report, save_modular_reports_to_results_dir
            
            # 1. ä¿å­˜åˆ†æ¨¡å—æŠ¥å‘Šåˆ°æœ¬åœ°ç›®å½•
            logger.info(f"ğŸ“ [æœ¬åœ°ä¿å­˜] å¼€å§‹ä¿å­˜åˆ†æ¨¡å—æŠ¥å‘Šåˆ°æœ¬åœ°ç›®å½•")
            local_files = save_modular_reports_to_results_dir(results, stock_symbol)
            if local_files:
                logger.info(f"âœ… [æœ¬åœ°ä¿å­˜] å·²ä¿å­˜ {len(local_files)} ä¸ªæœ¬åœ°æŠ¥å‘Šæ–‡ä»¶")
                for module, path in local_files.items():
                    logger.info(f"  - {module}: {path}")
            else:
                logger.warning(f"âš ï¸ [æœ¬åœ°ä¿å­˜] æœ¬åœ°æŠ¥å‘Šæ–‡ä»¶ä¿å­˜å¤±è´¥")
            
            # 2. ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°MongoDB
            logger.info(f"ğŸ—„ï¸ [MongoDBä¿å­˜] å¼€å§‹ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°MongoDB")
            save_success = save_analysis_report(
                stock_symbol=stock_symbol,
                analysis_results=results
            )
            
            if save_success:
                logger.info(f"âœ… [MongoDBä¿å­˜] åˆ†ææŠ¥å‘Šå·²æˆåŠŸä¿å­˜åˆ°MongoDB")
                update_progress("âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°æ•°æ®åº“å’Œæœ¬åœ°æ–‡ä»¶")
            else:
                logger.warning(f"âš ï¸ [MongoDBä¿å­˜] MongoDBæŠ¥å‘Šä¿å­˜å¤±è´¥")
                if local_files:
                    update_progress("âœ… æœ¬åœ°æŠ¥å‘Šå·²ä¿å­˜ï¼Œä½†æ•°æ®åº“ä¿å­˜å¤±è´¥")
                else:
                    update_progress("âš ï¸ æŠ¥å‘Šä¿å­˜å¤±è´¥ï¼Œä½†åˆ†æå·²å®Œæˆ")
                
        except Exception as save_error:
            logger.error(f"âŒ [æŠ¥å‘Šä¿å­˜] ä¿å­˜åˆ†ææŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {str(save_error)}")
            update_progress("âš ï¸ æŠ¥å‘Šä¿å­˜å‡ºé”™ï¼Œä½†åˆ†æå·²å®Œæˆ")

        update_progress("âœ… åˆ†ææˆåŠŸå®Œæˆï¼")
        return results

    except Exception as e:
        # è®°å½•åˆ†æå¤±è´¥çš„è¯¦ç»†æ—¥å¿—
        analysis_duration = time.time() - analysis_start_time

        logger_manager.log_module_error(
            logger, "comprehensive_analysis", stock_symbol, session_id,
            analysis_duration, str(e)
        )

        logger.error(f"âŒ [åˆ†æå¤±è´¥] è‚¡ç¥¨åˆ†ææ‰§è¡Œå¤±è´¥",
                    extra={
                        'stock_symbol': stock_symbol,
                        'session_id': session_id,
                        'duration': analysis_duration,
                        'error': str(e),
                        'error_type': type(e).__name__,
                        'analysts_used': analysts,
                        'success': False,
                        'event_type': 'web_analysis_error'
                    }, exc_info=True)

        # å¦‚æœçœŸå®åˆ†æå¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯è¯¯å¯¼æ€§æ¼”ç¤ºæ•°æ®
        return {
            'stock_symbol': stock_symbol,
            'analysis_date': analysis_date,
            'analysts': analysts,
            'research_depth': research_depth,
            'llm_provider': llm_provider,
            'llm_model': llm_model,
            'state': {},  # ç©ºçŠ¶æ€ï¼Œå°†æ˜¾ç¤ºå ä½ç¬¦
            'decision': {},  # ç©ºå†³ç­–
            'success': False,
            'error': str(e),
            'is_demo': False,
            'error_reason': f"åˆ†æå¤±è´¥: {str(e)}"
        }

def format_analysis_results(results):
    """æ ¼å¼åŒ–åˆ†æç»“æœç”¨äºæ˜¾ç¤º"""
    
    if not results['success']:
        return {
            'error': results['error'],
            'success': False
        }
    
    state = results['state']
    decision = results['decision']

    # æå–å…³é”®ä¿¡æ¯
    # decision å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚ "BUY", "SELL", "HOLD"ï¼‰æˆ–å­—å…¸
    if isinstance(decision, str):
        # å°†è‹±æ–‡æŠ•èµ„å»ºè®®è½¬æ¢ä¸ºä¸­æ–‡
        action_translation = {
            'BUY': 'ä¹°å…¥',
            'SELL': 'å–å‡º',
            'HOLD': 'æŒæœ‰',
            'buy': 'ä¹°å…¥',
            'sell': 'å–å‡º',
            'hold': 'æŒæœ‰'
        }
        action = action_translation.get(decision.strip(), decision.strip())

        formatted_decision = {
            'action': action,
            'confidence': 0.7,  # é»˜è®¤ç½®ä¿¡åº¦
            'risk_score': 0.3,  # é»˜è®¤é£é™©åˆ†æ•°
            'target_price': None,  # å­—ç¬¦ä¸²æ ¼å¼æ²¡æœ‰ç›®æ ‡ä»·æ ¼
            'reasoning': f'åŸºäºAIåˆ†æï¼Œå»ºè®®{decision.strip().upper()}'
        }
    elif isinstance(decision, dict):
        # å¤„ç†ç›®æ ‡ä»·æ ¼ - ç¡®ä¿æ­£ç¡®æå–æ•°å€¼
        target_price = decision.get('target_price')
        if target_price is not None and target_price != 'N/A':
            try:
                # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                if isinstance(target_price, str):
                    # ç§»é™¤è´§å¸ç¬¦å·å’Œç©ºæ ¼
                    clean_price = target_price.replace('$', '').replace('Â¥', '').replace('ï¿¥', '').strip()
                    target_price = float(clean_price) if clean_price and clean_price != 'None' else None
                elif isinstance(target_price, (int, float)):
                    target_price = float(target_price)
                else:
                    target_price = None
            except (ValueError, TypeError):
                target_price = None
        else:
            target_price = None

        # å°†è‹±æ–‡æŠ•èµ„å»ºè®®è½¬æ¢ä¸ºä¸­æ–‡
        action_translation = {
            'BUY': 'ä¹°å…¥',
            'SELL': 'å–å‡º',
            'HOLD': 'æŒæœ‰',
            'buy': 'ä¹°å…¥',
            'sell': 'å–å‡º',
            'hold': 'æŒæœ‰'
        }
        action = decision.get('action', 'æŒæœ‰')
        chinese_action = action_translation.get(action, action)

        formatted_decision = {
            'action': chinese_action,
            'confidence': decision.get('confidence', 0.5),
            'risk_score': decision.get('risk_score', 0.3),
            'target_price': target_price,
            'reasoning': decision.get('reasoning', 'æš‚æ— åˆ†ææ¨ç†')
        }
    else:
        # å¤„ç†å…¶ä»–ç±»å‹
        formatted_decision = {
            'action': 'æŒæœ‰',
            'confidence': 0.5,
            'risk_score': 0.3,
            'target_price': None,
            'reasoning': f'åˆ†æç»“æœ: {str(decision)}'
        }
    
    # æ ¼å¼åŒ–çŠ¶æ€ä¿¡æ¯
    formatted_state = {}
    
    # å¤„ç†å„ä¸ªåˆ†ææ¨¡å—çš„ç»“æœ - åŒ…å«å®Œæ•´çš„æ™ºèƒ½ä½“å›¢é˜Ÿåˆ†æ
    analysis_keys = [
        'market_report',
        'fundamentals_report',
        'sentiment_report',
        'news_report',
        'risk_assessment',
        'investment_plan',
        # æ·»åŠ ç¼ºå¤±çš„å›¢é˜Ÿå†³ç­–æ•°æ®ï¼Œç¡®ä¿ä¸CLIç«¯ä¸€è‡´
        'investment_debate_state',  # ç ”ç©¶å›¢é˜Ÿè¾©è®ºï¼ˆå¤šå¤´/ç©ºå¤´ç ”ç©¶å‘˜ï¼‰
        'trader_investment_plan',   # äº¤æ˜“å›¢é˜Ÿè®¡åˆ’
        'risk_debate_state',        # é£é™©ç®¡ç†å›¢é˜Ÿå†³ç­–
        'final_trade_decision'      # æœ€ç»ˆäº¤æ˜“å†³ç­–
    ]
    
    for key in analysis_keys:
        if key in state:
            # å¯¹æ–‡æœ¬å†…å®¹è¿›è¡Œä¸­æ–‡åŒ–å¤„ç†
            content = state[key]
            if isinstance(content, str):
                content = translate_analyst_labels(content)
            formatted_state[key] = content
        elif key == 'risk_assessment':
            # ç‰¹æ®Šå¤„ç†ï¼šä» risk_debate_state ç”Ÿæˆ risk_assessment
            risk_assessment = extract_risk_assessment(state)
            if risk_assessment:
                formatted_state[key] = risk_assessment
    
    return {
        'stock_symbol': results['stock_symbol'],
        'decision': formatted_decision,
        'state': formatted_state,
        'success': True,
        # å°†é…ç½®ä¿¡æ¯æ”¾åœ¨é¡¶å±‚ï¼Œä¾›å‰ç«¯ç›´æ¥è®¿é—®
        'analysis_date': results['analysis_date'],
        'analysts': results['analysts'],
        'research_depth': results['research_depth'],
        'llm_provider': results.get('llm_provider', 'dashscope'),
        'llm_model': results['llm_model'],
        'metadata': {
            'analysis_date': results['analysis_date'],
            'analysts': results['analysts'],
            'research_depth': results['research_depth'],
            'llm_provider': results.get('llm_provider', 'dashscope'),
            'llm_model': results['llm_model']
        }
    }

def validate_analysis_params(stock_symbol, analysis_date, analysts, research_depth, market_type="Aè‚¡"):
    """éªŒè¯åˆ†æå‚æ•°"""

    errors = []

    # éªŒè¯è‚¡ç¥¨ä»£ç 
    if not stock_symbol or len(stock_symbol.strip()) == 0:
        errors.append("è‚¡ç¥¨ä»£ç ä¸èƒ½ä¸ºç©º")
    elif len(stock_symbol.strip()) > 10:
        errors.append("è‚¡ç¥¨ä»£ç é•¿åº¦ä¸èƒ½è¶…è¿‡10ä¸ªå­—ç¬¦")
    else:
        # æ ¹æ®å¸‚åœºç±»å‹éªŒè¯ä»£ç æ ¼å¼
        symbol = stock_symbol.strip()
        if market_type == "Aè‚¡":
            # Aè‚¡ï¼š6ä½æ•°å­—
            import re
            if not re.match(r'^\d{6}$', symbol):
                errors.append("Aè‚¡ä»£ç æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º6ä½æ•°å­—ï¼ˆå¦‚ï¼š002115ï¼‰")
        elif market_type == "æ¸¯è‚¡":
            # æ¸¯è‚¡ï¼š4-5ä½æ•°å­—.HK æˆ– çº¯4-5ä½æ•°å­—
            import re
            symbol_upper = symbol.upper()
            # æ£€æŸ¥æ˜¯å¦ä¸º XXXX.HK æˆ– XXXXX.HK æ ¼å¼
            hk_format = re.match(r'^\d{4,5}\.HK$', symbol_upper)
            # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯4-5ä½æ•°å­—æ ¼å¼
            digit_format = re.match(r'^\d{4,5}$', symbol)

            if not (hk_format or digit_format):
                errors.append("æ¸¯è‚¡ä»£ç æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º4ä½æ•°å­—.HKï¼ˆå¦‚ï¼š0700.HKï¼‰æˆ–4ä½æ•°å­—ï¼ˆå¦‚ï¼š0700ï¼‰")
        elif market_type == "Aè‚¡":
            # ç¾è‚¡ï¼š1-5ä½å­—æ¯
            import re
            if not re.match(r'^[A-Z]{1,5}$', symbol.upper()):
                errors.append("ç¾è‚¡ä»£ç æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º1-5ä½å­—æ¯ï¼ˆå¦‚ï¼šAAPLï¼‰")
    
    # éªŒè¯åˆ†æå¸ˆåˆ—è¡¨
    if not analysts or len(analysts) == 0:
        errors.append("å¿…é¡»è‡³å°‘é€‰æ‹©ä¸€ä¸ªåˆ†æå¸ˆ")
    
    valid_analysts = ['market', 'social', 'news', 'fundamentals']
    invalid_analysts = [a for a in analysts if a not in valid_analysts]
    if invalid_analysts:
        errors.append(f"æ— æ•ˆçš„åˆ†æå¸ˆç±»å‹: {', '.join(invalid_analysts)}")
    
    # éªŒè¯ç ”ç©¶æ·±åº¦
    if not isinstance(research_depth, int) or research_depth < 1 or research_depth > 5:
        errors.append("ç ”ç©¶æ·±åº¦å¿…é¡»æ˜¯1-5ä¹‹é—´çš„æ•´æ•°")
    
    # éªŒè¯åˆ†ææ—¥æœŸ
    try:
        from datetime import datetime
        datetime.strptime(analysis_date, '%Y-%m-%d')
    except ValueError:
        errors.append("åˆ†ææ—¥æœŸæ ¼å¼æ— æ•ˆï¼Œåº”ä¸ºYYYY-MM-DDæ ¼å¼")
    
    return len(errors) == 0, errors

def get_supported_stocks():
    """è·å–æ”¯æŒçš„è‚¡ç¥¨åˆ—è¡¨"""
    
    # å¸¸è§çš„ç¾è‚¡è‚¡ç¥¨ä»£ç 
    popular_stocks = [
        {'symbol': 'AAPL', 'name': 'è‹¹æœå…¬å¸', 'sector': 'ç§‘æŠ€'},
        {'symbol': 'MSFT', 'name': 'å¾®è½¯', 'sector': 'ç§‘æŠ€'},
        {'symbol': 'GOOGL', 'name': 'è°·æ­Œ', 'sector': 'ç§‘æŠ€'},
        {'symbol': 'AMZN', 'name': 'äºšé©¬é€Š', 'sector': 'æ¶ˆè´¹'},
        {'symbol': 'TSLA', 'name': 'ç‰¹æ–¯æ‹‰', 'sector': 'æ±½è½¦'},
        {'symbol': 'NVDA', 'name': 'è‹±ä¼Ÿè¾¾', 'sector': 'ç§‘æŠ€'},
        {'symbol': 'META', 'name': 'Meta', 'sector': 'ç§‘æŠ€'},
        {'symbol': 'NFLX', 'name': 'å¥ˆé£', 'sector': 'åª’ä½“'},
        {'symbol': 'AMD', 'name': 'AMD', 'sector': 'ç§‘æŠ€'},
        {'symbol': 'INTC', 'name': 'è‹±ç‰¹å°”', 'sector': 'ç§‘æŠ€'},
        {'symbol': 'SPY', 'name': 'S&P 500 ETF', 'sector': 'ETF'},
        {'symbol': 'QQQ', 'name': 'çº³æ–¯è¾¾å…‹100 ETF', 'sector': 'ETF'},
    ]
    
    return popular_stocks

