# tradingagents/graph/parallel_analysts.py

import asyncio
import threading
import time
import copy
from typing import Dict, List, Any, Optional, Callable
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import get_logger
logger = get_logger("parallel_analysts")


@dataclass
class AnalystPerformance:
    """åˆ†æå¸ˆæ€§èƒ½è®°å½•"""
    analyst_type: str
    start_time: float
    end_time: float
    success: bool
    error_msg: Optional[str] = None
    report_length: int = 0

    @property
    def duration(self) -> float:
        return self.end_time - self.start_time


class ParallelAnalystsExecutor:
    """å¹¶è¡Œåˆ†æå¸ˆæ‰§è¡Œå™¨"""

    def __init__(self, max_workers: int = 4, timeout: float = 300):
        """
        åˆå§‹åŒ–å¹¶è¡Œæ‰§è¡Œå™¨

        Args:
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
            timeout: å•ä¸ªåˆ†æå¸ˆè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.max_workers = max_workers
        self.timeout = timeout
        self.performances: List[AnalystPerformance] = []
        self.results_lock = threading.Lock()

    def execute_parallel(self, analyst_nodes: Dict[str, Callable],
                        state: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰åˆ†æå¸ˆ

        Args:
            analyst_nodes: åˆ†æå¸ˆèŠ‚ç‚¹å­—å…¸ {"analyst_type": analyst_function}
            state: å…±äº«çŠ¶æ€

        Returns:
            æ›´æ–°åçš„çŠ¶æ€å­—å…¸
        """
        logger.info(f"ğŸš€ [å¹¶è¡Œæ‰§è¡Œå™¨] å¼€å§‹å¹¶è¡Œæ‰§è¡Œ {len(analyst_nodes)} ä¸ªåˆ†æå¸ˆ")
        start_time = time.time()

        # æ¸…ç©ºä¹‹å‰çš„æ€§èƒ½è®°å½•
        self.performances = []

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
        results = {}
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_analyst = {}
            for analyst_type, analyst_node in analyst_nodes.items():
                # ä¸ºæ¯ä¸ªåˆ†æå¸ˆåˆ›å»ºçŠ¶æ€å‰¯æœ¬ï¼Œé¿å…ç«äº‰æ¡ä»¶
                state_copy = self._deep_copy_state(state)
                future = executor.submit(
                    self._execute_single_analyst,
                    analyst_type,
                    analyst_node,
                    state_copy
                )
                future_to_analyst[future] = analyst_type

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_analyst.keys(), timeout=self.timeout + 30):
                analyst_type = future_to_analyst[future]
                try:
                    result = future.result(timeout=10)  # çŸ­è¶…æ—¶ï¼Œå› ä¸ºä»»åŠ¡åº”è¯¥å·²å®Œæˆ
                    results[analyst_type] = result
                except Exception as e:
                    logger.error(f"âŒ [å¹¶è¡Œæ‰§è¡Œå™¨] {analyst_type} åˆ†æå¸ˆæ‰§è¡Œå¤±è´¥: {str(e)}")
                    results[analyst_type] = None

        # åˆå¹¶ç»“æœ
        merged_state = self._merge_analyst_results(state, results)

        total_time = time.time() - start_time
        success_count = len([r for r in results.values() if r is not None])

        logger.info(f"âœ… [å¹¶è¡Œæ‰§è¡Œå™¨] å¹¶è¡Œæ‰§è¡Œå®Œæˆï¼Œè€—æ—¶ {total_time:.2f}ç§’")
        logger.info(f"ğŸ“Š [å¹¶è¡Œæ‰§è¡Œå™¨] æˆåŠŸç‡: {success_count}/{len(analyst_nodes)} ({success_count/len(analyst_nodes)*100:.1f}%)")

        return merged_state

    def _execute_single_analyst(self, analyst_type: str, analyst_node: Callable,
                               state: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æ‰§è¡Œå•ä¸ªåˆ†æå¸ˆ"""
        start_time = time.time()

        try:
            logger.info(f"ğŸ”„ [{analyst_type.capitalize()} Analyst] å¼€å§‹å¹¶è¡Œæ‰§è¡Œ")

            # è®¾ç½®è¶…æ—¶æ‰§è¡Œ
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(analyst_node, state)
                result = future.result(timeout=self.timeout)

            end_time = time.time()

            # æå–æŠ¥å‘Šé•¿åº¦
            report_length = 0
            if result:
                for key in ['market_report', 'sentiment_report', 'news_report',
                           'fundamentals_report', 'china_market_report']:
                    if key in result and result[key]:
                        report_length = len(str(result[key]))
                        break

            # è®°å½•æ€§èƒ½
            perf = AnalystPerformance(
                analyst_type=analyst_type,
                start_time=start_time,
                end_time=end_time,
                success=True,
                report_length=report_length
            )
            with self.results_lock:
                self.performances.append(perf)

            logger.info(f"âœ… [{analyst_type.capitalize()} Analyst] å¹¶è¡Œæ‰§è¡Œå®Œæˆï¼Œè€—æ—¶ {end_time - start_time:.2f}ç§’ï¼ŒæŠ¥å‘Šé•¿åº¦ {report_length}")
            return result

        except Exception as e:
            end_time = time.time()
            error_msg = str(e)

            # è®°å½•å¤±è´¥æ€§èƒ½
            perf = AnalystPerformance(
                analyst_type=analyst_type,
                start_time=start_time,
                end_time=end_time,
                success=False,
                error_msg=error_msg
            )
            with self.results_lock:
                self.performances.append(perf)

            logger.error(f"âŒ [{analyst_type.capitalize()} Analyst] æ‰§è¡Œå¼‚å¸¸: {error_msg}")
            return None

    def _deep_copy_state(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """æ·±æ‹·è´çŠ¶æ€ï¼Œé¿å…å¹¶å‘ä¿®æ”¹å†²çª"""
        try:
            return copy.deepcopy(state)
        except Exception as e:
            logger.warning(f"âš ï¸ [å¹¶è¡Œæ‰§è¡Œå™¨] æ·±æ‹·è´çŠ¶æ€å¤±è´¥ï¼Œä½¿ç”¨æµ…æ‹·è´: {e}")
            return copy.copy(state)

    def _merge_analyst_results(self, original_state: Dict[str, Any],
                              results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆå¹¶åˆ†æå¸ˆç»“æœ"""
        merged_state = original_state.copy()

        # æ”¶é›†æ‰€æœ‰æ¶ˆæ¯
        all_messages = list(merged_state.get("messages", []))

        # åˆå¹¶å„åˆ†æå¸ˆçš„æŠ¥å‘Š
        for analyst_type, result in results.items():
            if result is None:
                continue

            # æ·»åŠ æ¶ˆæ¯
            if "messages" in result:
                all_messages.extend(result["messages"])

            # æ ¹æ®åˆ†æå¸ˆç±»å‹æ›´æ–°å¯¹åº”çš„æŠ¥å‘Šå­—æ®µ
            if analyst_type == "market" and "market_report" in result:
                merged_state["market_report"] = result["market_report"]
            elif analyst_type == "social" and "sentiment_report" in result:
                merged_state["sentiment_report"] = result["sentiment_report"]
            elif analyst_type == "news" and "news_report" in result:
                merged_state["news_report"] = result["news_report"]
            elif analyst_type == "fundamentals" and "fundamentals_report" in result:
                merged_state["fundamentals_report"] = result["fundamentals_report"]
            elif analyst_type == "china_market" and "china_market_report" in result:
                merged_state["china_market_report"] = result["china_market_report"]

            # æ›´æ–°å‘é€è€…ä¿¡æ¯
            if "sender" in result:
                merged_state["sender"] = result["sender"]

        # æ›´æ–°æ¶ˆæ¯åˆ—è¡¨
        merged_state["messages"] = all_messages

        successful_analysts = [k for k, v in results.items() if v is not None]
        logger.info(f"ğŸ”„ [å¹¶è¡Œæ‰§è¡Œå™¨] ç»“æœåˆå¹¶å®Œæˆï¼ŒæˆåŠŸæ‰§è¡Œçš„åˆ†æå¸ˆ: {successful_analysts}")

        return merged_state

    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ€»ç»“"""
        if not self.performances:
            return {"status": "no_data"}

        total_time = max(p.end_time for p in self.performances) - min(p.start_time for p in self.performances)
        success_count = sum(1 for p in self.performances if p.success)

        return {
            "total_parallel_time": round(total_time, 2),
            "success_rate": round(success_count / len(self.performances), 2),
            "successful_count": success_count,
            "total_count": len(self.performances),
            "analyst_performances": [
                {
                    "analyst": p.analyst_type,
                    "duration": round(p.duration, 2),
                    "success": p.success,
                    "error": p.error_msg,
                    "report_length": p.report_length
                }
                for p in self.performances
            ]
        }


def create_parallel_analysts_node(analyst_nodes: Dict[str, Callable],
                                 config: Dict[str, Any]) -> Callable:
    """
    åˆ›å»ºå¹¶è¡Œåˆ†æå¸ˆèŠ‚ç‚¹

    Args:
        analyst_nodes: åˆ†æå¸ˆèŠ‚ç‚¹å­—å…¸
        config: é…ç½®å­—å…¸

    Returns:
        å¹¶è¡Œåˆ†æå¸ˆæ‰§è¡Œå‡½æ•°
    """
    max_workers = config.get("max_parallel_workers", 4)
    timeout = config.get("analyst_timeout", 300)

    executor = ParallelAnalystsExecutor(max_workers=max_workers, timeout=timeout)

    def parallel_analysts_node(state: Dict[str, Any]) -> Dict[str, Any]:
        """å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰åˆ†æå¸ˆçš„èŠ‚ç‚¹"""
        logger.info(f"ğŸ“Š [å¹¶è¡Œåˆ†æå¸ˆèŠ‚ç‚¹] å¼€å§‹æ‰§è¡Œï¼Œé…ç½®: max_workers={max_workers}, timeout={timeout}s")

        # æ‰§è¡Œå¹¶è¡Œåˆ†æ
        result_state = executor.execute_parallel(analyst_nodes, state)

        # æ·»åŠ æ€§èƒ½æ•°æ®åˆ°çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
        performance_summary = executor.get_performance_summary()
        result_state["parallel_performance"] = performance_summary

        logger.info(f"ğŸ“Š [å¹¶è¡Œåˆ†æå¸ˆèŠ‚ç‚¹] æ‰§è¡Œå®Œæˆï¼Œæ€§èƒ½æ€»ç»“: {performance_summary}")

        return result_state

    return parallel_analysts_node