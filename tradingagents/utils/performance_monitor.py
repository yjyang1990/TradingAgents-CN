"""
æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–å·¥å…·

ä¸ºADataå¸‚åœºæ•°æ®é›†æˆæä¾›æ€§èƒ½ç›‘æ§ã€åˆ†æå’Œä¼˜åŒ–å»ºè®®åŠŸèƒ½ã€‚

Author: Claude (Opus 4.1)
Created: 2025-09-25
"""

import time
import psutil
import threading
from datetime import datetime, timedelta
from collections import defaultdict, deque
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import json
import logging

logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    timestamp: float
    function_name: str
    execution_time: float
    memory_before: float
    memory_after: float
    memory_delta: float
    cache_hit: bool
    data_size: int
    error_occurred: bool
    error_message: Optional[str] = None


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self, max_history: int = 1000):
        self.max_history = max_history
        self.metrics_history: deque = deque(maxlen=max_history)
        self.function_stats: Dict[str, List[PerformanceMetrics]] = defaultdict(list)
        self.cache_stats: Dict[str, Dict[str, int]] = defaultdict(lambda: {"hits": 0, "misses": 0})
        self.error_stats: Dict[str, int] = defaultdict(int)
        self.lock = threading.RLock()

    def record_metric(self, metric: PerformanceMetrics):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        with self.lock:
            self.metrics_history.append(metric)
            self.function_stats[metric.function_name].append(metric)

            # é™åˆ¶æ¯ä¸ªå‡½æ•°çš„å†å²è®°å½•æ•°é‡
            if len(self.function_stats[metric.function_name]) > 100:
                self.function_stats[metric.function_name] = \
                    self.function_stats[metric.function_name][-100:]

            # æ›´æ–°ç¼“å­˜ç»Ÿè®¡
            if metric.cache_hit:
                self.cache_stats[metric.function_name]["hits"] += 1
            else:
                self.cache_stats[metric.function_name]["misses"] += 1

            # æ›´æ–°é”™è¯¯ç»Ÿè®¡
            if metric.error_occurred:
                self.error_stats[metric.function_name] += 1

    def get_function_performance(self, function_name: str) -> Dict[str, Any]:
        """è·å–æŒ‡å®šå‡½æ•°çš„æ€§èƒ½ç»Ÿè®¡"""
        with self.lock:
            metrics = self.function_stats.get(function_name, [])
            if not metrics:
                return {"error": f"No metrics found for function {function_name}"}

            execution_times = [m.execution_time for m in metrics if not m.error_occurred]
            memory_deltas = [m.memory_delta for m in metrics if not m.error_occurred]

            if not execution_times:
                return {"error": f"No successful executions for function {function_name}"}

            cache_stats = self.cache_stats[function_name]
            total_cache_ops = cache_stats["hits"] + cache_stats["misses"]
            cache_hit_rate = (cache_stats["hits"] / total_cache_ops * 100) if total_cache_ops > 0 else 0

            return {
                "function_name": function_name,
                "total_calls": len(metrics),
                "successful_calls": len(execution_times),
                "error_count": self.error_stats[function_name],
                "avg_execution_time": sum(execution_times) / len(execution_times),
                "min_execution_time": min(execution_times),
                "max_execution_time": max(execution_times),
                "avg_memory_delta": sum(memory_deltas) / len(memory_deltas),
                "cache_hit_rate": cache_hit_rate,
                "last_call": metrics[-1].timestamp if metrics else None
            }

    def get_overall_performance(self) -> Dict[str, Any]:
        """è·å–æ•´ä½“æ€§èƒ½ç»Ÿè®¡"""
        with self.lock:
            if not self.metrics_history:
                return {"error": "No performance data available"}

            total_calls = len(self.metrics_history)
            successful_calls = sum(1 for m in self.metrics_history if not m.error_occurred)
            total_errors = sum(1 for m in self.metrics_history if m.error_occurred)

            execution_times = [m.execution_time for m in self.metrics_history if not m.error_occurred]

            # è®¡ç®—æ•´ä½“ç¼“å­˜å‘½ä¸­ç‡
            total_hits = sum(stats["hits"] for stats in self.cache_stats.values())
            total_cache_ops = sum(stats["hits"] + stats["misses"] for stats in self.cache_stats.values())
            overall_cache_hit_rate = (total_hits / total_cache_ops * 100) if total_cache_ops > 0 else 0

            # æ‰¾å‡ºæœ€æ…¢çš„å‡½æ•°
            function_avg_times = {}
            for func_name, metrics in self.function_stats.items():
                successful_metrics = [m for m in metrics if not m.error_occurred]
                if successful_metrics:
                    avg_time = sum(m.execution_time for m in successful_metrics) / len(successful_metrics)
                    function_avg_times[func_name] = avg_time

            slowest_functions = sorted(function_avg_times.items(), key=lambda x: x[1], reverse=True)[:5]

            return {
                "monitoring_period": {
                    "start_time": self.metrics_history[0].timestamp,
                    "end_time": self.metrics_history[-1].timestamp,
                    "duration_hours": (self.metrics_history[-1].timestamp - self.metrics_history[0].timestamp) / 3600
                },
                "call_statistics": {
                    "total_calls": total_calls,
                    "successful_calls": successful_calls,
                    "error_calls": total_errors,
                    "success_rate": (successful_calls / total_calls * 100) if total_calls > 0 else 0
                },
                "performance_statistics": {
                    "avg_execution_time": sum(execution_times) / len(execution_times) if execution_times else 0,
                    "min_execution_time": min(execution_times) if execution_times else 0,
                    "max_execution_time": max(execution_times) if execution_times else 0
                },
                "cache_statistics": {
                    "overall_cache_hit_rate": overall_cache_hit_rate,
                    "total_cache_operations": total_cache_ops
                },
                "top_functions": {
                    "slowest_functions": slowest_functions,
                    "most_called_functions": [(func, len(metrics)) for func, metrics in
                                            sorted(self.function_stats.items(), key=lambda x: len(x[1]), reverse=True)[:5]]
                }
            }

    def get_performance_insights(self) -> List[str]:
        """è·å–æ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        insights = []
        overall_stats = self.get_overall_performance()

        if "error" in overall_stats:
            return ["æ— è¶³å¤Ÿæ•°æ®è¿›è¡Œæ€§èƒ½åˆ†æ"]

        # ç¼“å­˜å‘½ä¸­ç‡åˆ†æ
        cache_hit_rate = overall_stats["cache_statistics"]["overall_cache_hit_rate"]
        if cache_hit_rate < 70:
            insights.append(f"âš ï¸ ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½({cache_hit_rate:.1f}%)ï¼Œå»ºè®®ä¼˜åŒ–ç¼“å­˜ç­–ç•¥")
        elif cache_hit_rate > 90:
            insights.append(f"âœ… ç¼“å­˜å‘½ä¸­ç‡ä¼˜ç§€({cache_hit_rate:.1f}%)")

        # é”™è¯¯ç‡åˆ†æ
        success_rate = overall_stats["call_statistics"]["success_rate"]
        if success_rate < 95:
            insights.append(f"âš ï¸ æˆåŠŸç‡è¾ƒä½({success_rate:.1f}%)ï¼Œå»ºè®®æ£€æŸ¥é”™è¯¯å¤„ç†æœºåˆ¶")

        # æ€§èƒ½åˆ†æ
        avg_time = overall_stats["performance_statistics"]["avg_execution_time"]
        if avg_time > 2.0:
            insights.append(f"âš ï¸ å¹³å‡æ‰§è¡Œæ—¶é—´è¾ƒé•¿({avg_time:.2f}ç§’)ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®è·å–é€»è¾‘")

        # æ…¢å‡½æ•°åˆ†æ
        slowest_functions = overall_stats["top_functions"]["slowest_functions"]
        if slowest_functions and slowest_functions[0][1] > 5.0:
            func_name, avg_time = slowest_functions[0]
            insights.append(f"ğŸŒ æœ€æ…¢å‡½æ•° {func_name} å¹³å‡è€—æ—¶ {avg_time:.2f}ç§’ï¼Œå»ºè®®ä¼˜å…ˆä¼˜åŒ–")

        # è°ƒç”¨é¢‘ç‡åˆ†æ
        most_called = overall_stats["top_functions"]["most_called_functions"]
        if most_called:
            func_name, call_count = most_called[0]
            insights.append(f"ğŸ”¥ æœ€é¢‘ç¹è°ƒç”¨çš„å‡½æ•°æ˜¯ {func_name}({call_count}æ¬¡è°ƒç”¨)ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨å…¶æ€§èƒ½")

        if not insights:
            insights.append("âœ… ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œæ— æ˜æ˜¾ä¼˜åŒ–ç‚¹")

        return insights

    def export_report(self, filepath: str) -> bool:
        """å¯¼å‡ºæ€§èƒ½æŠ¥å‘Š"""
        try:
            report_data = {
                "export_time": datetime.now().isoformat(),
                "overall_performance": self.get_overall_performance(),
                "function_details": {
                    func_name: self.get_function_performance(func_name)
                    for func_name in self.function_stats.keys()
                },
                "performance_insights": self.get_performance_insights()
            }

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)

            logger.info(f"æ€§èƒ½æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {filepath}")
            return True
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")
            return False


class PerformanceProfiler:
    """æ€§èƒ½åˆ†æè£…é¥°å™¨"""

    def __init__(self, monitor: PerformanceMonitor):
        self.monitor = monitor

    def profile(self, function_name: Optional[str] = None):
        """æ€§èƒ½åˆ†æè£…é¥°å™¨"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                fname = function_name or func.__name__

                # è·å–æ‰§è¡Œå‰å†…å­˜
                process = psutil.Process()
                memory_before = process.memory_info().rss / 1024 / 1024  # MB

                start_time = time.time()
                error_occurred = False
                error_message = None
                result = None
                data_size = 0
                cache_hit = False

                try:
                    result = func(*args, **kwargs)

                    # å°è¯•æ£€æµ‹ç¼“å­˜å‘½ä¸­ï¼ˆåŸºäºæ‰§è¡Œæ—¶é—´å¯å‘å¼åˆ¤æ–­ï¼‰
                    execution_time = time.time() - start_time
                    if execution_time < 0.1:  # å°äº100mså¯èƒ½æ˜¯ç¼“å­˜å‘½ä¸­
                        cache_hit = True

                    # ä¼°ç®—æ•°æ®å¤§å°
                    if isinstance(result, str):
                        data_size = len(result)
                    elif hasattr(result, '__len__'):
                        try:
                            data_size = len(str(result))
                        except:
                            data_size = 0

                except Exception as e:
                    error_occurred = True
                    error_message = str(e)
                    result = None

                # è·å–æ‰§è¡Œåå†…å­˜
                memory_after = process.memory_info().rss / 1024 / 1024  # MB
                execution_time = time.time() - start_time

                # è®°å½•æ€§èƒ½æŒ‡æ ‡
                metric = PerformanceMetrics(
                    timestamp=time.time(),
                    function_name=fname,
                    execution_time=execution_time,
                    memory_before=memory_before,
                    memory_after=memory_after,
                    memory_delta=memory_after - memory_before,
                    cache_hit=cache_hit,
                    data_size=data_size,
                    error_occurred=error_occurred,
                    error_message=error_message
                )

                self.monitor.record_metric(metric)

                if error_occurred:
                    raise Exception(error_message)

                return result

            return wrapper
        return decorator


class SystemResourceMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""

    def __init__(self):
        self.monitoring = False
        self.monitor_thread = None
        self.resource_history: deque = deque(maxlen=1000)

    def start_monitoring(self, interval: float = 5.0):
        """å¼€å§‹ç³»ç»Ÿèµ„æºç›‘æ§"""
        if self.monitoring:
            return

        self.monitoring = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_resources,
            args=(interval,),
            daemon=True
        )
        self.monitor_thread.start()
        logger.info("ç³»ç»Ÿèµ„æºç›‘æ§å·²å¯åŠ¨")

    def stop_monitoring(self):
        """åœæ­¢ç³»ç»Ÿèµ„æºç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
        logger.info("ç³»ç»Ÿèµ„æºç›‘æ§å·²åœæ­¢")

    def _monitor_resources(self, interval: float):
        """èµ„æºç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                # è·å–ç³»ç»Ÿèµ„æºä¿¡æ¯
                cpu_percent = psutil.cpu_percent(interval=1)
                memory = psutil.virtual_memory()
                disk = psutil.disk_usage('/')

                resource_data = {
                    "timestamp": time.time(),
                    "cpu_percent": cpu_percent,
                    "memory_percent": memory.percent,
                    "memory_available_gb": memory.available / (1024**3),
                    "disk_percent": disk.percent,
                    "disk_free_gb": disk.free / (1024**3)
                }

                self.resource_history.append(resource_data)

                # å¦‚æœèµ„æºä½¿ç”¨ç‡è¿‡é«˜ï¼Œè®°å½•è­¦å‘Š
                if cpu_percent > 80:
                    logger.warning(f"CPUä½¿ç”¨ç‡è¿‡é«˜: {cpu_percent}%")
                if memory.percent > 85:
                    logger.warning(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory.percent}%")
                if disk.percent > 90:
                    logger.warning(f"ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: {disk.percent}%")

                time.sleep(interval)

            except Exception as e:
                logger.error(f"èµ„æºç›‘æ§å‡ºé”™: {e}")
                time.sleep(interval)

    def get_resource_summary(self) -> Dict[str, Any]:
        """è·å–èµ„æºä½¿ç”¨æ€»ç»“"""
        if not self.resource_history:
            return {"error": "No resource monitoring data available"}

        recent_data = list(self.resource_history)[-60:]  # æœ€è¿‘60ä¸ªæ•°æ®ç‚¹

        cpu_values = [data["cpu_percent"] for data in recent_data]
        memory_values = [data["memory_percent"] for data in recent_data]

        return {
            "monitoring_period": {
                "start_time": recent_data[0]["timestamp"],
                "end_time": recent_data[-1]["timestamp"],
                "data_points": len(recent_data)
            },
            "cpu_statistics": {
                "average": sum(cpu_values) / len(cpu_values),
                "maximum": max(cpu_values),
                "minimum": min(cpu_values)
            },
            "memory_statistics": {
                "average_percent": sum(memory_values) / len(memory_values),
                "maximum_percent": max(memory_values),
                "current_available_gb": recent_data[-1]["memory_available_gb"]
            },
            "disk_statistics": {
                "current_usage_percent": recent_data[-1]["disk_percent"],
                "current_free_gb": recent_data[-1]["disk_free_gb"]
            }
        }


# å…¨å±€æ€§èƒ½ç›‘æ§å®ä¾‹
global_performance_monitor = PerformanceMonitor()
global_profiler = PerformanceProfiler(global_performance_monitor)
global_resource_monitor = SystemResourceMonitor()


def get_performance_dashboard() -> str:
    """è·å–æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿"""
    try:
        overall_stats = global_performance_monitor.get_overall_performance()
        insights = global_performance_monitor.get_performance_insights()
        resource_summary = global_resource_monitor.get_resource_summary()

        dashboard = f"""
# ğŸ¯ TradingAgents-CN æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿
*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*

## ğŸ“Š æ•´ä½“æ€§èƒ½ç»Ÿè®¡
"""

        if "error" not in overall_stats:
            call_stats = overall_stats["call_statistics"]
            perf_stats = overall_stats["performance_statistics"]
            cache_stats = overall_stats["cache_statistics"]

            dashboard += f"""
### è°ƒç”¨ç»Ÿè®¡
- **æ€»è°ƒç”¨æ¬¡æ•°**: {call_stats['total_calls']}
- **æˆåŠŸè°ƒç”¨**: {call_stats['successful_calls']}
- **å¤±è´¥è°ƒç”¨**: {call_stats['error_calls']}
- **æˆåŠŸç‡**: {call_stats['success_rate']:.1f}%

### æ€§èƒ½æŒ‡æ ‡
- **å¹³å‡æ‰§è¡Œæ—¶é—´**: {perf_stats['avg_execution_time']:.3f}ç§’
- **æœ€å¿«æ‰§è¡Œ**: {perf_stats['min_execution_time']:.3f}ç§’
- **æœ€æ…¢æ‰§è¡Œ**: {perf_stats['max_execution_time']:.3f}ç§’

### ç¼“å­˜ç»Ÿè®¡
- **ç¼“å­˜å‘½ä¸­ç‡**: {cache_stats['overall_cache_hit_rate']:.1f}%
- **æ€»ç¼“å­˜æ“ä½œ**: {cache_stats['total_cache_operations']}
"""

        dashboard += "\n## ğŸ¯ æ€§èƒ½ä¼˜åŒ–å»ºè®®\n"
        for i, insight in enumerate(insights, 1):
            dashboard += f"{i}. {insight}\n"

        # èµ„æºç›‘æ§ä¿¡æ¯
        dashboard += "\n## ğŸ’» ç³»ç»Ÿèµ„æºçŠ¶å†µ\n"
        if "error" not in resource_summary:
            cpu_stats = resource_summary["cpu_statistics"]
            mem_stats = resource_summary["memory_statistics"]
            disk_stats = resource_summary["disk_statistics"]

            dashboard += f"""
### CPUä½¿ç”¨æƒ…å†µ
- **å¹³å‡ä½¿ç”¨ç‡**: {cpu_stats['average']:.1f}%
- **å³°å€¼ä½¿ç”¨ç‡**: {cpu_stats['maximum']:.1f}%

### å†…å­˜ä½¿ç”¨æƒ…å†µ
- **å¹³å‡ä½¿ç”¨ç‡**: {mem_stats['average_percent']:.1f}%
- **å½“å‰å¯ç”¨å†…å­˜**: {mem_stats['current_available_gb']:.2f}GB

### ç£ç›˜ä½¿ç”¨æƒ…å†µ
- **å½“å‰ä½¿ç”¨ç‡**: {disk_stats['current_usage_percent']:.1f}%
- **å‰©ä½™ç©ºé—´**: {disk_stats['current_free_gb']:.2f}GB
"""
        else:
            dashboard += "ç³»ç»Ÿèµ„æºç›‘æ§æ•°æ®ä¸å¯ç”¨\n"

        dashboard += f"""
---
*ç›‘æ§èŒƒå›´*: ADataå¸‚åœºæ•°æ®é›†æˆç³»ç»Ÿ
*æ•°æ®æ¥æº*: æ€§èƒ½ç›‘æ§å™¨ + ç³»ç»Ÿèµ„æºç›‘æ§å™¨
"""

        return dashboard

    except Exception as e:
        return f"âš ï¸ ç”Ÿæˆæ€§èƒ½ä»ªè¡¨æ¿å¤±è´¥: {str(e)}"


def initialize_performance_monitoring():
    """åˆå§‹åŒ–æ€§èƒ½ç›‘æ§ç³»ç»Ÿ"""
    try:
        # å¯åŠ¨ç³»ç»Ÿèµ„æºç›‘æ§
        global_resource_monitor.start_monitoring(interval=10.0)
        logger.info("âœ… æ€§èƒ½ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"âŒ æ€§èƒ½ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        return False


def cleanup_performance_monitoring():
    """æ¸…ç†æ€§èƒ½ç›‘æ§ç³»ç»Ÿ"""
    try:
        global_resource_monitor.stop_monitoring()
        logger.info("âœ… æ€§èƒ½ç›‘æ§ç³»ç»Ÿæ¸…ç†å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ æ€§èƒ½ç›‘æ§ç³»ç»Ÿæ¸…ç†å¤±è´¥: {e}")


if __name__ == "__main__":
    # æµ‹è¯•æ€§èƒ½ç›‘æ§ç³»ç»Ÿ
    initialize_performance_monitoring()

    print("æ€§èƒ½ç›‘æ§ç³»ç»Ÿæµ‹è¯•...")
    time.sleep(2)

    # æ¨¡æ‹Ÿä¸€äº›æ€§èƒ½æ•°æ®
    @global_profiler.profile("test_function")
    def test_function():
        time.sleep(0.1)
        return "test_result" * 100

    # æ‰§è¡Œæµ‹è¯•å‡½æ•°å‡ æ¬¡
    for _ in range(5):
        test_function()

    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    dashboard = get_performance_dashboard()
    print(dashboard)

    cleanup_performance_monitoring()