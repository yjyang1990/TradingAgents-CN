#!/usr/bin/env python3
"""
LLMé€‚é…å™¨å¢å¼ºå™¨
ä¸ºç°æœ‰çš„LLMé€‚é…å™¨åŠ¨æ€æ·»åŠ è¯¦ç»†çš„AIè°ƒç”¨æ—¥å¿—è®°å½•åŠŸèƒ½
"""

import functools
import inspect
from typing import Any, Callable, Type, Optional
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

from tradingagents.utils.ai_call_logger import log_ai_call, log_ai_debug
from tradingagents.utils.logging_init import get_logger

logger = get_logger("llm_enhancer")


def enhance_llm_with_logging(
    llm_instance: Any,
    provider: str,
    model: str = None,
    enable_detailed_logging: bool = True
) -> Any:
    """
    ä¸ºLLMå®ä¾‹åŠ¨æ€æ·»åŠ è¯¦ç»†çš„AIè°ƒç”¨æ—¥å¿—è®°å½•

    Args:
        llm_instance: LLMå®ä¾‹
        provider: æä¾›å•†åç§°
        model: æ¨¡å‹åç§°
        enable_detailed_logging: æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿—è®°å½•

    Returns:
        å¢å¼ºåçš„LLMå®ä¾‹
    """

    # è·å–æ¨¡å‹åç§°
    actual_model = model
    if not actual_model and hasattr(llm_instance, 'model'):
        actual_model = llm_instance.model
    elif not actual_model and hasattr(llm_instance, 'model_name'):
        actual_model = llm_instance.model_name

    logger.info(f"ğŸ”§ [LLMå¢å¼ºå™¨] ä¸º {provider}/{actual_model} æ·»åŠ è¯¦ç»†æ—¥å¿—è®°å½•")

    # å¢å¼º _generate æ–¹æ³•
    if hasattr(llm_instance, '_generate') and callable(getattr(llm_instance, '_generate')):
        original_generate = getattr(llm_instance, '_generate')

        # å¦‚æœè¿˜æ²¡æœ‰è¢«è£…é¥°è¿‡
        if not hasattr(original_generate, '_ai_logging_enhanced'):
            if enable_detailed_logging:
                enhanced_generate = log_ai_call(
                    provider=provider,
                    model=actual_model,
                    log_input=True,
                    log_output=True,
                    log_tokens=True
                )(original_generate)
            else:
                enhanced_generate = log_ai_call(
                    provider=provider,
                    model=actual_model,
                    log_input=False,
                    log_output=False,
                    log_tokens=True
                )(original_generate)

            enhanced_generate._ai_logging_enhanced = True
            setattr(llm_instance, '_generate', enhanced_generate)
            logger.debug(f"âœ… [LLMå¢å¼ºå™¨] {provider}/{actual_model} _generateæ–¹æ³•å·²å¢å¼º")

    # å¢å¼º invoke æ–¹æ³• - ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼å¤„ç†Pydanticæ¨¡å‹
    if hasattr(llm_instance, 'invoke') and callable(getattr(llm_instance, 'invoke')):
        original_invoke = getattr(llm_instance, 'invoke')

        # å¦‚æœè¿˜æ²¡æœ‰è¢«è£…é¥°è¿‡
        if not hasattr(original_invoke, '_ai_logging_enhanced'):
            if enable_detailed_logging:
                enhanced_invoke = log_ai_call(
                    provider=provider,
                    model=actual_model,
                    log_input=True,
                    log_output=True,
                    log_tokens=True
                )(original_invoke)
            else:
                enhanced_invoke = log_ai_call(
                    provider=provider,
                    model=actual_model,
                    log_input=False,
                    log_output=False,
                    log_tokens=True
                )(original_invoke)

            enhanced_invoke._ai_logging_enhanced = True

            # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼è®¾ç½®æ–¹æ³•ï¼Œå¤„ç†Pydanticæ¨¡å‹çš„é™åˆ¶
            try:
                # å°è¯•ç›´æ¥è®¾ç½®
                setattr(llm_instance, 'invoke', enhanced_invoke)
                logger.debug(f"âœ… [LLMå¢å¼ºå™¨] {provider}/{actual_model} invokeæ–¹æ³•å·²å¢å¼º")
            except (ValueError, AttributeError) as e:
                # å¦‚æœæ˜¯Pydanticæ¨¡å‹ï¼Œå°è¯•ä½¿ç”¨__dict__ç›´æ¥è®¾ç½®
                if hasattr(llm_instance, '__dict__'):
                    llm_instance.__dict__['invoke'] = enhanced_invoke
                    logger.debug(f"âœ… [LLMå¢å¼ºå™¨] {provider}/{actual_model} invokeæ–¹æ³•å·²å¢å¼º (ä½¿ç”¨__dict__)")
                else:
                    logger.warning(f"âš ï¸ [LLMå¢å¼ºå™¨] æ— æ³•å¢å¼º {provider}/{actual_model} invokeæ–¹æ³•: {e}")
                    # ä¸é˜»æ­¢ç»§ç»­å¤„ç†å…¶ä»–æ–¹æ³•

    # å¢å¼º generate æ–¹æ³•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if hasattr(llm_instance, 'generate') and callable(getattr(llm_instance, 'generate')):
        original_generate_method = getattr(llm_instance, 'generate')

        if not hasattr(original_generate_method, '_ai_logging_enhanced'):
            if enable_detailed_logging:
                enhanced_generate_method = log_ai_call(
                    provider=provider,
                    model=actual_model,
                    log_input=True,
                    log_output=True,
                    log_tokens=True
                )(original_generate_method)
            else:
                enhanced_generate_method = log_ai_call(
                    provider=provider,
                    model=actual_model,
                    log_input=False,
                    log_output=False,
                    log_tokens=True
                )(original_generate_method)

            enhanced_generate_method._ai_logging_enhanced = True

            # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼è®¾ç½®æ–¹æ³•
            try:
                setattr(llm_instance, 'generate', enhanced_generate_method)
                logger.debug(f"âœ… [LLMå¢å¼ºå™¨] {provider}/{actual_model} generateæ–¹æ³•å·²å¢å¼º")
            except (ValueError, AttributeError) as e:
                if hasattr(llm_instance, '__dict__'):
                    llm_instance.__dict__['generate'] = enhanced_generate_method
                    logger.debug(f"âœ… [LLMå¢å¼ºå™¨] {provider}/{actual_model} generateæ–¹æ³•å·²å¢å¼º (ä½¿ç”¨__dict__)")
                else:
                    logger.warning(f"âš ï¸ [LLMå¢å¼ºå™¨] æ— æ³•å¢å¼º {provider}/{actual_model} generateæ–¹æ³•: {e}")

    # ä¸ºå®ä¾‹æ·»åŠ å¢å¼ºæ ‡è®° - ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼
    try:
        llm_instance._ai_logging_enhanced = True
        llm_instance._enhanced_provider = provider
        llm_instance._enhanced_model = actual_model
    except (ValueError, AttributeError):
        # å¯¹äºPydanticæ¨¡å‹ï¼Œä½¿ç”¨__dict__
        if hasattr(llm_instance, '__dict__'):
            llm_instance.__dict__['_ai_logging_enhanced'] = True
            llm_instance.__dict__['_enhanced_provider'] = provider
            llm_instance.__dict__['_enhanced_model'] = actual_model

    return llm_instance


def enhance_openai_llm(llm_instance: ChatOpenAI, enable_detailed_logging: bool = True) -> ChatOpenAI:
    """ä¸ºOpenAI LLMå®ä¾‹æ·»åŠ è¯¦ç»†æ—¥å¿—è®°å½•"""
    return enhance_llm_with_logging(
        llm_instance=llm_instance,
        provider="openai",
        enable_detailed_logging=enable_detailed_logging
    )


def enhance_anthropic_llm(llm_instance: ChatAnthropic, enable_detailed_logging: bool = True) -> ChatAnthropic:
    """ä¸ºAnthropic LLMå®ä¾‹æ·»åŠ è¯¦ç»†æ—¥å¿—è®°å½•"""
    return enhance_llm_with_logging(
        llm_instance=llm_instance,
        provider="anthropic",
        enable_detailed_logging=enable_detailed_logging
    )


def enhance_all_llm_methods(llm_instance: Any, provider: str) -> Any:
    """
    ä½¿ç”¨è°ƒè¯•æ¨¡å¼å¢å¼ºLLMå®ä¾‹çš„æ‰€æœ‰ç›¸å…³æ–¹æ³•
    è‡ªåŠ¨æ£€æµ‹å¹¶è£…é¥°æ‰€æœ‰AIè°ƒç”¨ç›¸å…³çš„æ–¹æ³•
    """

    logger.info(f"ğŸ” [LLMå…¨é¢å¢å¼º] å¼€å§‹ä¸º {provider} LLMå®ä¾‹æ·»åŠ è°ƒè¯•æ—¥å¿—")

    # éœ€è¦å¢å¼ºçš„æ–¹æ³•åˆ—è¡¨
    methods_to_enhance = [
        '_generate', 'invoke', 'generate', 'chat', '_chat',
        'predict', '_predict', 'call', '_call', 'stream',
        '_stream', 'agenerate', '_agenerate', 'ainvoke'
    ]

    enhanced_count = 0

    for method_name in methods_to_enhance:
        if hasattr(llm_instance, method_name):
            method = getattr(llm_instance, method_name)
            if callable(method) and not hasattr(method, '_ai_logging_enhanced'):
                # ä½¿ç”¨è°ƒè¯•æ¨¡å¼è£…é¥°å™¨
                enhanced_method = log_ai_debug()(method)
                enhanced_method._ai_logging_enhanced = True
                setattr(llm_instance, method_name, enhanced_method)
                enhanced_count += 1
                logger.debug(f"âœ… [LLMå…¨é¢å¢å¼º] {provider} {method_name}æ–¹æ³•å·²å¢å¼º")

    if enhanced_count > 0:
        logger.info(f"ğŸ¯ [LLMå…¨é¢å¢å¼º] å·²ä¸º {provider} LLMå®ä¾‹å¢å¼º {enhanced_count} ä¸ªæ–¹æ³•")
        llm_instance._ai_debug_enhanced = True
    else:
        logger.warning(f"âš ï¸ [LLMå…¨é¢å¢å¼º] æœªæ‰¾åˆ°å¯å¢å¼ºçš„æ–¹æ³• for {provider}")

    return llm_instance


def create_enhanced_llm_factory(
    original_llm_class: Type,
    provider: str,
    enable_auto_enhancement: bool = True
):
    """
    åˆ›å»ºè‡ªåŠ¨å¢å¼ºçš„LLMå·¥å‚ç±»

    Args:
        original_llm_class: åŸå§‹LLMç±»
        provider: æä¾›å•†åç§°
        enable_auto_enhancement: æ˜¯å¦å¯ç”¨è‡ªåŠ¨å¢å¼º

    Returns:
        å¢å¼ºåçš„LLMå·¥å‚ç±»
    """

    class EnhancedLLM(original_llm_class):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)

            if enable_auto_enhancement:
                logger.info(f"ğŸ­ [LLMå·¥å‚] è‡ªåŠ¨å¢å¼º {provider} LLMå®ä¾‹")
                enhance_llm_with_logging(
                    llm_instance=self,
                    provider=provider,
                    enable_detailed_logging=True
                )

    EnhancedLLM.__name__ = f"Enhanced{original_llm_class.__name__}"
    EnhancedLLM.__qualname__ = f"Enhanced{original_llm_class.__name__}"

    return EnhancedLLM


# ä¾¿æ·å‡½æ•°ï¼šæ‰¹é‡å¢å¼ºLLMå®ä¾‹
def batch_enhance_llms(llm_instances: dict, enable_detailed_logging: bool = True) -> dict:
    """
    æ‰¹é‡å¢å¼ºå¤šä¸ªLLMå®ä¾‹

    Args:
        llm_instances: LLMå®ä¾‹å­—å…¸ï¼Œæ ¼å¼: {"provider_name": llm_instance}
        enable_detailed_logging: æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿—è®°å½•

    Returns:
        å¢å¼ºåçš„LLMå®ä¾‹å­—å…¸
    """

    enhanced_instances = {}

    for provider, llm_instance in llm_instances.items():
        logger.info(f"ğŸ”„ [æ‰¹é‡å¢å¼º] æ­£åœ¨å¢å¼º {provider} LLMå®ä¾‹")

        try:
            enhanced_instance = enhance_llm_with_logging(
                llm_instance=llm_instance,
                provider=provider,
                enable_detailed_logging=enable_detailed_logging
            )
            enhanced_instances[provider] = enhanced_instance
            logger.info(f"âœ… [æ‰¹é‡å¢å¼º] {provider} LLMå®ä¾‹å¢å¼ºæˆåŠŸ")

        except Exception as e:
            logger.error(f"âŒ [æ‰¹é‡å¢å¼º] {provider} LLMå®ä¾‹å¢å¼ºå¤±è´¥: {e}")
            enhanced_instances[provider] = llm_instance  # ä½¿ç”¨åŸå§‹å®ä¾‹

    logger.info(f"ğŸ¯ [æ‰¹é‡å¢å¼º] å®Œæˆï¼Œå…±å¢å¼º {len(enhanced_instances)} ä¸ªLLMå®ä¾‹")
    return enhanced_instances


def is_llm_enhanced(llm_instance: Any) -> bool:
    """æ£€æŸ¥LLMå®ä¾‹æ˜¯å¦å·²è¢«å¢å¼º"""
    return hasattr(llm_instance, '_ai_logging_enhanced') and llm_instance._ai_logging_enhanced


def get_llm_enhancement_info(llm_instance: Any) -> dict:
    """è·å–LLMå®ä¾‹çš„å¢å¼ºä¿¡æ¯"""
    if not is_llm_enhanced(llm_instance):
        return {"enhanced": False}

    return {
        "enhanced": True,
        "provider": getattr(llm_instance, '_enhanced_provider', 'unknown'),
        "model": getattr(llm_instance, '_enhanced_model', 'unknown'),
        "debug_enhanced": getattr(llm_instance, '_ai_debug_enhanced', False)
    }